{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path \n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "home = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../sentimentModelDump\", \"rb\") as fp:\n",
    "    model, vec = pickle.load(fp)\n",
    "\n",
    "print(model, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command at the terminal and type in words and hit enter periodically:\n",
      "nc -lk 4015\n"
     ]
    }
   ],
   "source": [
    "PORT=4015 # Change this to a unique port before running individually\n",
    "HOST=\"localhost\"\n",
    "\n",
    "print(\"Run this command at the terminal and type in words and hit enter periodically:\")\n",
    "print(f\"nc -lk {PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.DataFrame({\"Text\" : [\"This is very negative tweet\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vec.transform(testDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"ItemID\", IntegerType(), True), \\\n",
    "    StructField(\"Sentiment\", IntegerType(), True), \\\n",
    "    StructField(\"SentimentText\",StringType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets just check to make sure that the model still works...\n",
    "\n",
    "1: Positive Sentiment <br>\n",
    "0: Negative Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new historical test data: 0.7223846153846154\n"
     ]
    }
   ],
   "source": [
    "test_spark_df = spark.read.schema(schema).csv(f'{home}/csc-369-student/data/twitter_sentiment_analysis/historical/xb*')\n",
    "historical_test_data = test_spark_df.toPandas()\n",
    "test_tf_sparse = vec.transform(historical_test_data['SentimentText'])\n",
    "print(\"Accuracy on new historical test data:\",sum(model.predict(test_tf_sparse) == historical_test_data['Sentiment'])/len(historical_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch\n",
    "def test(line):\n",
    "    df = pd.DataFrame({\"Tweet\" : [line]})\n",
    "    pred = model.predict(vec.transform(testDF))\n",
    "\n",
    "\n",
    "    return spark.createDataFrame([(\"Positive:\" if pred == 1 else \"Negative\", line)])\n",
    "    # line = \"{} {}\".format(\n",
    "    #             \"Positive:\" \n",
    "    #             if pred == 1 else \n",
    "    #             \"Negative\", \n",
    "                \n",
    "    #             line)\n",
    "\n",
    "    \n",
    "\n",
    "    return line\n",
    "\n",
    "# tweets = lines.map(test)\n",
    "tweets = lines.map(lambda line: pd.DataFrame({\"Tweet\" : [line]}))\n",
    "# tweets = lines.map(lambda line: model.predict(vec.transform(testDF)))\n",
    "tweets = lines.map(lambda line: model.predict(vec.transform(line)))\n",
    "# tweets = lines.map(lambda line: line + \"Positive:\" if model.predict(vec.transform(line)) == 1 else \"Negative\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_row2(line):\n",
    "        print(\"HELLO WORLD\")\n",
    "        print(\"Line: \", line)\n",
    "        print(\"SentimentText: \" , line)\n",
    "        return Row(SentimentText=line)\n",
    "\n",
    "def process(time, rdd):\n",
    "\n",
    "\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    else:\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        print('hello')\n",
    "        print(\"Starting\")\n",
    "        rowRdd = rdd.map(line_to_row2)\n",
    "            \n",
    "        # print(\"row RDD: \", rowRdd)\n",
    "        df_rows = spark.createDataFrame(rowRdd)\n",
    "        print(\"df_rows: \", df_rows)\n",
    "        df_pandas = df_rows.toPandas()\n",
    "        print(\"I AM ALIVE\")\n",
    "        print(\"df_pandas\", df_pandas.values())\n",
    "        # model.pre\n",
    "    # except Exception:\n",
    "    #     print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-11-17 18:13:59 =========\n",
      "========= 2021-11-17 18:14:00 =========\n",
      "========= 2021-11-17 18:14:01 =========\n",
      "========= 2021-11-17 18:14:02 =========\n",
      "========= 2021-11-17 18:14:03 =========\n",
      "========= 2021-11-17 18:14:04 =========\n",
      "========= 2021-11-17 18:14:05 =========\n",
      "========= 2021-11-17 18:14:06 =========\n",
      "========= 2021-11-17 18:14:07 =========\n",
      "========= 2021-11-17 18:14:08 =========\n",
      "========= 2021-11-17 18:14:09 =========\n",
      "========= 2021-11-17 18:14:10 =========\n",
      "========= 2021-11-17 18:14:11 =========\n",
      "========= 2021-11-17 18:14:12 =========\n",
      "hello\n",
      "Starting\n",
      "df_rows:  DataFrame[SentimentText: string]\n",
      "========= 2021-11-17 18:14:13 =========\n",
      "hello\n",
      "Starting\n",
      "df_rows:  DataFrame[SentimentText: string]\n",
      "========= 2021-11-17 18:14:14 =========\n",
      "hello\n",
      "Starting\n",
      "df_rows:  DataFrame[SentimentText: string]\n",
      "========= 2021-11-17 18:14:15 =========\n",
      "========= 2021-11-17 18:14:16 =========\n",
      "========= 2021-11-17 18:14:17 =========\n",
      "========= 2021-11-17 18:14:18 =========\n",
      "========= 2021-11-17 18:14:19 =========\n",
      "========= 2021-11-17 18:14:20 =========\n",
      "========= 2021-11-17 18:14:21 =========\n",
      "========= 2021-11-17 18:14:22 =========\n",
      "========= 2021-11-17 18:14:23 =========\n",
      "========= 2021-11-17 18:14:24 =========\n",
      "========= 2021-11-17 18:14:25 =========\n",
      "========= 2021-11-17 18:14:26 =========\n",
      "========= 2021-11-17 18:14:27 =========\n",
      "========= 2021-11-17 18:14:28 =========\n"
     ]
    }
   ],
   "source": [
    "HOST = \"localhost\"\n",
    "PORT = 4015\n",
    "lines = ssc.socketTextStream(HOST, PORT)\n",
    "\n",
    "lines.foreachRDD(process)\n",
    "# tweets.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:45\n",
      "-------------------------------------------\n",
      "hello world\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:51\n",
      "-------------------------------------------\n",
      "hello world\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:54\n",
      "-------------------------------------------\n",
      "hello world\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:04:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:02\n",
      "-------------------------------------------\n",
      "hello world\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:04\n",
      "-------------------------------------------\n",
      "hello world\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-17 18:05:08\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(line):\n",
    "    print(\"hello world\")\n",
    "    return \"hello world\"\n",
    "    # df = pd.DataFrame({\"Tweet\" : [line]})\n",
    "    # pred = model.predict(vec.transform(testDF))\n",
    "\n",
    "\n",
    "    # return spark.createDataFrame([(\"Positive:\" if pred == 1 else \"Negative\", line)])\n",
    "    # # line = \"{} {}\".format(\n",
    "    # #             \"Positive:\" \n",
    "    # #             if pred == 1 else \n",
    "    # #             \"Negative\", \n",
    "                \n",
    "    # #             line)\n",
    "\n",
    "HOST = \"localhost\"\n",
    "PORT = 4015\n",
    "lines = ssc.socketTextStream(HOST, PORT)\n",
    "\n",
    "tweets = lines.map(test)\n",
    "tweets.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09a709a61535e3979084f22402a67a55a38dd5d3d5f5feeec9f7472303b7fba4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
